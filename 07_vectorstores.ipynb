{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** Retrieval-Augmented Generation with Vector Stores</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about embedding models and exercised some of their capabilities. We discussed their intended use cases of longer-form document comparison and found ways to use it as a backbone for more custom semantic comparisons. This notebook will progress these ideas toward the retrieval model's intended use case and explore how to build chatbot systems that rely on *vector stores* to automatically save and retrieve information.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Understand how semantic-similarity-backed systems can facilitate easy-to-use retrieval formulations.\n",
    "\n",
    "- Learn how to incorporate retrieval modules into your chat model systems for a retrieval-augmented generation (RAG) pipeline, which can be applied to tasks like document retrieval and conversation memory buffers.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- This notebook does not attempt to incorporate hierarchical reasoning or non-naive RAG (such as planning agents). Consider what modifications would be necessary to make these components work in an LCEL chain.\n",
    "\n",
    "- Consider when it would be best to move your vector store solution into a scalable service and when a GPU will become necessary for optimization.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1075336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## Part 1: Summary of RAG Workflows\n",
    "\n",
    "This notebook will explore several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted):\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Vector Store Workflow for Conversational Exchanges:***\n",
    "- Generate semantic embedding for each new conversation.\n",
    "- Add the message body to a vector store for retrieval.\n",
    "- Query the vector store for relevant messages to fill in the LLM context.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Modified Workflow for an Arbitrary Document:***\n",
    "- **Divide the document into chunks and process them into useful messages.**\n",
    "- Generate semantic embedding for each **new document chunk**.\n",
    "- Add the **chunk bodies** to a vector store for retrieval.\n",
    "- Query the vector store for relevant **chunks** to fill in the LLM context.\n",
    "    - ***Optional:* Modify/synthesize results for better LLM results.**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Extended Workflow for a Directory of Arbitrary Documents:**\n",
    "- Divide **each document** into chunks and process them into useful messages.\n",
    "- Generate semantic embedding for each new document chunk.\n",
    "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
    "    - ***Optional*: Exploit hierarchical or metadata structures for larger systems.**\n",
    "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
    "    - *Optional:* Modify/synthesize results for better LLM results.\n",
    "\n",
    "<br>\n",
    "\n",
    "Some of the most important terminology surrounding RAG is covered in detail on the [**LlamaIndex Concepts page**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html), which itself is a great starting point for progressing towards the LlamaIndex loading and retrieving strategy. We highly recommend using it as a reference as you continue with this notebook and advise you to try out LlamaIndex after the course to consider the pros and cons firsthand!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** RAG for Conversation History\n",
    "\n",
    "In our previous explorations, we delved into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **Step 1**: Getting A Conversation\n",
    "\n",
    "Consider a conversation crafted using Llama-13B between a chat agent and a blue bear named Beras. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **Step 2:** Constructing Our Vector Store Retriever\n",
    "\n",
    "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ZjwYbSZzsXK6ZP8O1-cY3BeRffV4oqzb\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "In addition to simplifying the process from an API perspective, vector stores also implement connectors, integrations, and optimizations under the hood. In our case, we will start with the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss), which integrates a LangChain-compatable Embedding model with the [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss) library to make the process fast and scalable on our local machine!\n",
    "\n",
    "**Specifically:**\n",
    "\n",
    "1. We can feed our conversation into [**a FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss) via the `from_texts` constructor. This will take our conversational data and the embedding model to create a searchable index over our discussion.\n",
    "2. This vector store can then be \"interpreted\" as a retriever, supporting the LangChain runnable API and returning documents retrieved via an input query.\n",
    "\n",
    "The following shows how you can construct a FAISS vector store and reinterpret it as a retriever using the LangChain `vectorstore` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:56\u001b[39m, in \u001b[36mdependable_faiss_import\u001b[39m\u001b[34m(no_avx2)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:6\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:996\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__from\u001b[39m(\n\u001b[32m    986\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    994\u001b[39m     **kwargs: Any,\n\u001b[32m    995\u001b[39m ) -> FAISS:\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     faiss = \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:\n\u001b[32m    998\u001b[39m         index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:58\u001b[39m, in \u001b[36mdependable_faiss_import\u001b[39m\u001b[34m(no_avx2)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import faiss python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[31mImportError\u001b[39m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "The retriever can now be used like any other LangChain runnable to query the vector store for some relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'e353d1dc-ea7e-4d2e-8ce2-e74cdcfb4806'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'a56695c1-f8cf-42e7-ae72-89a1227b69dc'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'52be62fa-4efa-47e6-b94b-25eff9bd5cbb'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'294bc0b1-3f82-4b80-bf79-2a1e37e54f2e'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'e353d1dc-ea7e-4d2e-8ce2-e74cdcfb4806'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'a56695c1-f8cf-42e7-ae72-89a1227b69dc'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'52be62fa-4efa-47e6-b94b-25eff9bd5cbb'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001b[0m\n",
       "\u001b[32myou!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'294bc0b1-3f82-4b80-bf79-2a1e37e54f2e'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'b4fe4535-c52f-4024-877e-906302adb393'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'294bc0b1-3f82-4b80-bf79-2a1e37e54f2e'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'e353d1dc-ea7e-4d2e-8ce2-e74cdcfb4806'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'a56695c1-f8cf-42e7-ae72-89a1227b69dc'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'b4fe4535-c52f-4024-877e-906302adb393'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'294bc0b1-3f82-4b80-bf79-2a1e37e54f2e'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'e353d1dc-ea7e-4d2e-8ce2-e74cdcfb4806'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'a56695c1-f8cf-42e7-ae72-89a1227b69dc'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "As we can see, our retriever found a handful of semantically relevant documents from our query. You may notice that not all of the documents are useful or clear on their own. For example, a retrieval of *\"Beras\"* for *\"your name\"* may be problematic for the chatbot if provided out of context. Anticipating the potential problems and creating synergies between your LLM components can increase the likelihood of good RAG behavior, so keep an eye out for such pitfalls and opportunities.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
    "\n",
    "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
    "- **A retriever is always retrieving context by default**.\n",
    "- **A generator is acting on the retrieved context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the context provided, Beras lives in the Arctic. It seems like the Rocky Mountains are a fascinating place</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for Beras to learn about, considering the vastly different climate from where they currently reside!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on the context provided, Beras lives in the Arctic. It seems like the Rocky Mountains are a fascinating place\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor Beras to learn about, considering the vastly different climate from where they currently reside!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "Take a second to try out some more invocations and see how the new setup performs. Regardless of your model choice, the following questions should serve as interesting starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without more context, I can't give you a precise location beyond that. However, you can find out more by doing some</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">research online or watching documentaries about them. I hope you enjoy learning about the Rocky Mountains!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithout more context, I can't give you a precise location beyond that. However, you can find out more by doing some\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresearch online or watching documentaries about them. I hope you enjoy learning about the Rocky Mountains!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. To answer your </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question about their location, they do not directly border California. In fact, they run north to south, starting </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">from western Canada and stretching all the way down through the states of Montana, Idaho, Wyoming, Colorado, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into New Mexico. So, while they're not close to California, they do cover a significant portion of North America! </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like to know more about the Rocky Mountains?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. To answer your \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion about their location, they do not directly border California. In fact, they run north to south, starting \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfrom western Canada and stretching all the way down through the states of Montana, Idaho, Wyoming, Colorado, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minto New Mexico. So, while they're not close to California, they do cover a significant portion of North America! \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mWould you like to know more about the Rocky Mountains?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">context doesn't provide details about the exact location or the distance between the Arctic and the Rocky </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. The Rocky Mountains stretch across North America, but without specific information, it's difficult to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provide an exact distance.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontext doesn't provide details about the exact location or the distance between the Arctic and the Rocky \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. The Rocky Mountains stretch across North America, but without specific information, it's difficult to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovide an exact distance.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>\n",
    "\n",
    "You might notice some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **Step 4:** Automatic Conversation Storage\n",
    "\n",
    "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's worth noting </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the Rocky Mountains are known more for their stunning landscapes, diverse wildlife, and opportunities for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outdoor activities like hiking, camping, and skiing. Even so, I'm glad you're excited about the possibility of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">visiting this beautiful place!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's worth noting \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat the Rocky Mountains are known more for their stunning landscapes, diverse wildlife, and opportunities for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutdoor activities like hiking, camping, and skiing. Even so, I'm glad you're excited about the possibility of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvisiting this beautiful place!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well, based on our conversation, it seems like you're quite fond of ice cream! Enjoying it in beautiful places like</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Rocky Mountains sounds like a wonderful plan. Is ice cream your favorite food, Beras?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWell, based on our conversation, it seems like you're quite fond of ice cream! Enjoying it in beautiful places like\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe Rocky Mountains sounds like a wonderful plan. Is ice cream your favorite food, Beras?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Ah, my apologies, Beras! I suppose I took a guess based on our conversation and the image of you enjoying ice cream</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in the beautiful Rocky Mountains. Honey is a wonderful choice, too! I didn't know you had a sweet spot for honey. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Is there anything special you like to have it with?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAh, my apologies, Beras! I suppose I took a guess based on our conversation and the image of you enjoying ice cream\u001b[0m\n",
       "\u001b[1;38;2;118;185;0min the beautiful Rocky Mountains. Honey is a wonderful choice, too! I didn't know you had a sweet spot for honey. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mIs there anything special you like to have it with?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on our recent conversation, I now know that your favorite food is indeed honey! It's a fantastic choice. I'm </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">really intrigued, is there a specific way you like to enjoy your honey or particular dishes you pair it with?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on our recent conversation, I now know that your favorite food is indeed honey! It's a fantastic choice. I'm \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreally intrigued, is there a specific way you like to enjoy your honey or particular dishes you pair it with?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3 [Exercise]:** RAG For Document Chunk Retrieval\n",
    "\n",
    "Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn't surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may **seem** to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let's see what we can do!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Exercise:**\n",
    "\n",
    "In the previous example, you may recall that we pulled in some relatively small papers with the help of [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) using the following syntax:\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "Given all that you've learned so far, choose a selection of papers that you would like to use and develop a chatbot that can talk about them!\n",
    "\n",
    "<br>\n",
    "\n",
    "Though this is a pretty big task, a walkthrough of ***most*** of the process will be provided below. By the end of the walkthrough, many of the necessary puzzle pieces will be provided, and your real task will be to integrate them together for the final `retrieval_chain`. When you're done, get ready to re-integrate the chain (or a flavor of your choice) in the last notebook as part of the evaluation exercise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1**: Loading And Chunking Your Documents\n",
    "\n",
    "The following code block gives you some default papers to load in for your RAG chain. Feel free to select more papers as desired, but note that longer documents will take longer to process. A few simplifying assumptions and additional processing steps are included to help you improve your naive RAG performance:\n",
    "\n",
    "- Documents are cut off prior to the \"References\" section if one exists. This will keep our system from considering the citations and appendix sections, which tend to be long and distracting.\n",
    "\n",
    "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. If your pipeline does not provide metadata on each retrieval, this is a useful component and can even be listed among a list of higher-priority pieces if appropriate.\n",
    "\n",
    "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks.\n",
    "\n",
    "**NOTE:** ***For the sake of the assessment, please include at least one paper that is less than one month old!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Large Language Models: A Survey</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Generative AI in the Construction Industry: A State-of-the-art Analysis</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Financial Report Chunking for Effective Retrieval Augmented Generation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - AI4Contracts: LLM &amp; RAG-Powered Encoding of Financial Derivative Contracts</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Generative AI in the Construction Industry: A State-of-the-art Analysis\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Financial Report Chunking for Effective Retrieval Augmented Generation\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative Contracts\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
       "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
       "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
       "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
       "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
       "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
       "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
       "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
       "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
       "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
       "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
       "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
       "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
       "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
       "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
       "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
       "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
       "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
       "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
       "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
       "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
       "\u001b[32mHeinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
       "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
       "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
       "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
       "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
       "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
       "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
       "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
       "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
       "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
       "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
       "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
       "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
       "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
       "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
       "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
       "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
       "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
       "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
       "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
       "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
       "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
       "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
       "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
       "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, \u001b[0m\n",
       "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
       "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
       "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
       "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
       "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
       "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
       "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
       "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
       "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
       "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
       "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
       "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
       "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
       "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
       "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
       "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
       "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
       "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
       "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
       "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 122\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-03-27'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Large Language Models: A Survey'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wang, Haofen Wang'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs'</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">article delineates the challenges currently faced\\nand points out prospective avenues for research and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">development.\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-03-27'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Large Language Models: A Survey'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng \u001b[0m\n",
       "\u001b[32mWang, Haofen Wang'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m showcase impressive capabilities but encounter\\nchallenges like \u001b[0m\n",
       "\u001b[32mhallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented \u001b[0m\n",
       "\u001b[32mGeneration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This \u001b[0m\n",
       "\u001b[32menhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows \u001b[0m\n",
       "\u001b[32mfor continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs'\u001b[0m\n",
       "\u001b[32mintrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper \u001b[0m\n",
       "\u001b[32moffers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, \u001b[0m\n",
       "\u001b[32mand the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the \u001b[0m\n",
       "\u001b[32mretrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies\u001b[0m\n",
       "\u001b[32membedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG \u001b[0m\n",
       "\u001b[32msystems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this \u001b[0m\n",
       "\u001b[32marticle delineates the challenges currently faced\\nand points out prospective avenues for research and \u001b[0m\n",
       "\u001b[32mdevelopment.\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 7\n",
      " - # Chunks: 86\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-08-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large language models (LLMs) have achieved remarkable success due to their\\nexceptional generative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities. Despite their success, they also have\\ninherent limitations such as a lack of up-to-date knowledge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and hallucination.\\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\\nmitigate these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations. The key idea of RAG is to ground the answer\\ngeneration of an LLM on external knowledge retrieved from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a knowledge database.\\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\\nleaving its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">security largely unexplored. We aim to bridge the gap in this work.\\nWe find that the knowledge database in a RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system introduces a new and\\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\\nthe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">first knowledge corruption attack to RAG, where an attacker could inject a\\nfew malicious texts into the knowledge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">database of a RAG system to induce an\\nLLM to generate an attacker-chosen target answer for an attacker-chosen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target\\nquestion. We formulate knowledge corruption attacks as an optimization problem,\\nwhose solution is a set of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">malicious texts. Depending on the background\\nknowledge (e.g., black-box and white-box settings) of an attacker on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a RAG\\nsystem, we propose two solutions to solve the optimization problem,\\nrespectively. Our results show </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PoisonedRAG could achieve a 90% attack success\\nrate when injecting five malicious texts for each target question </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">into a\\nknowledge database with millions of texts. We also evaluate several defenses\\nand our results show they are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">insufficient to defend against PoisonedRAG,\\nhighlighting the need for new defenses.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-08-13'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language \u001b[0m\n",
       "\u001b[32mModels'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have achieved remarkable success due to their\\nexceptional generative \u001b[0m\n",
       "\u001b[32mcapabilities. Despite their success, they also have\\ninherent limitations such as a lack of up-to-date knowledge \u001b[0m\n",
       "\u001b[32mand hallucination.\\nRetrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a state-of-the-art technique to\\nmitigate these \u001b[0m\n",
       "\u001b[32mlimitations. The key idea of RAG is to ground the answer\\ngeneration of an LLM on external knowledge retrieved from\u001b[0m\n",
       "\u001b[32ma knowledge database.\\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\\nleaving its \u001b[0m\n",
       "\u001b[32msecurity largely unexplored. We aim to bridge the gap in this work.\\nWe find that the knowledge database in a RAG \u001b[0m\n",
       "\u001b[32msystem introduces a new and\\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\\nthe \u001b[0m\n",
       "\u001b[32mfirst knowledge corruption attack to RAG, where an attacker could inject a\\nfew malicious texts into the knowledge \u001b[0m\n",
       "\u001b[32mdatabase of a RAG system to induce an\\nLLM to generate an attacker-chosen target answer for an attacker-chosen \u001b[0m\n",
       "\u001b[32mtarget\\nquestion. We formulate knowledge corruption attacks as an optimization problem,\\nwhose solution is a set of\u001b[0m\n",
       "\u001b[32mmalicious texts. Depending on the background\\nknowledge \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., black-box and white-box settings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of an attacker on \u001b[0m\n",
       "\u001b[32ma RAG\\nsystem, we propose two solutions to solve the optimization problem,\\nrespectively. Our results show \u001b[0m\n",
       "\u001b[32mPoisonedRAG could achieve a 90% attack success\\nrate when injecting five malicious texts for each target question \u001b[0m\n",
       "\u001b[32minto a\\nknowledge database with millions of texts. We also evaluate several defenses\\nand our results show they are\u001b[0m\n",
       "\u001b[32minsufficient to defend against PoisonedRAG,\\nhighlighting the need for new defenses.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 8\n",
      " - # Chunks: 36\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-02-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large Language Models~(LLMs) have gained immense popularity and are being\\nincreasingly applied in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">various domains. Consequently, ensuring the security of\\nthese models is of paramount importance. Jailbreak </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attacks, which manipulate\\nLLMs to generate malicious content, are recognized as a significant\\nvulnerability. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">While existing research has predominantly focused on direct\\njailbreak attacks on LLMs, there has been limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploration of indirect\\nmethods. The integration of various plugins into LLMs, notably Retrieval\\nAugmented </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Generation~(RAG), which enables LLMs to incorporate external\\nknowledge bases into their response generation such </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as GPTs, introduces new\\navenues for indirect jailbreak attacks.\\n  To fill this gap, we investigate indirect </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">jailbreak attacks on LLMs,\\nparticularly GPTs, introducing a novel attack vector named Retrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Augmented\\nGeneration Poisoning. This method, Pandora, exploits the synergy between LLMs\\nand RAG through prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">manipulation to generate unexpected responses. Pandora\\nuses maliciously crafted content to influence the RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process, effectively\\ninitiating jailbreak attacks. Our preliminary tests show that Pandora\\nsuccessfully conducts </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">jailbreak attacks in four different scenarios, achieving\\nhigher success rates than direct attacks, with 64.3\\\\% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for GPT-3.5 and 34.8\\\\%\\nfor GPT-4.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-02-13'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large Language Models~\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have gained immense popularity and are being\\nincreasingly applied in \u001b[0m\n",
       "\u001b[32mvarious domains. Consequently, ensuring the security of\\nthese models is of paramount importance. Jailbreak \u001b[0m\n",
       "\u001b[32mattacks, which manipulate\\nLLMs to generate malicious content, are recognized as a significant\\nvulnerability. \u001b[0m\n",
       "\u001b[32mWhile existing research has predominantly focused on direct\\njailbreak attacks on LLMs, there has been limited \u001b[0m\n",
       "\u001b[32mexploration of indirect\\nmethods. The integration of various plugins into LLMs, notably Retrieval\\nAugmented \u001b[0m\n",
       "\u001b[32mGeneration~\u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which enables LLMs to incorporate external\\nknowledge bases into their response generation such \u001b[0m\n",
       "\u001b[32mas GPTs, introduces new\\navenues for indirect jailbreak attacks.\\n  To fill this gap, we investigate indirect \u001b[0m\n",
       "\u001b[32mjailbreak attacks on LLMs,\\nparticularly GPTs, introducing a novel attack vector named Retrieval \u001b[0m\n",
       "\u001b[32mAugmented\\nGeneration Poisoning. This method, Pandora, exploits the synergy between LLMs\\nand RAG through prompt \u001b[0m\n",
       "\u001b[32mmanipulation to generate unexpected responses. Pandora\\nuses maliciously crafted content to influence the RAG \u001b[0m\n",
       "\u001b[32mprocess, effectively\\ninitiating jailbreak attacks. Our preliminary tests show that Pandora\\nsuccessfully conducts \u001b[0m\n",
       "\u001b[32mjailbreak attacks in four different scenarios, achieving\\nhigher success rates than direct attacks, with 64.3\\\\% \u001b[0m\n",
       "\u001b[32mfor GPT-3.5 and 34.8\\\\%\\nfor GPT-4.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 9\n",
      " - # Chunks: 133\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-02-15'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Generative AI in the Construction Industry: A State-of-the-art Analysis'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Salami, Abdullahi Saka, Tarek Zayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The construction industry is a vital sector of the global economy, but it\\nfaces many productivity </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">challenges in various processes, such as design,\\nplanning, procurement, inspection, and maintenance. Generative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">artificial\\nintelligence (AI), which can create novel and realistic data or content, such\\nas text, image, video, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or code, based on some input or prior knowledge, offers\\ninnovative and disruptive solutions to address these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">challenges. However, there\\nis a gap in the literature on the current state, opportunities, and challenges\\nof </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative AI in the construction industry. This study aims to fill this gap\\nby providing a state-of-the-art </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analysis of generative AI in construction, with\\nthree objectives: (1) to review and categorize the existing and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">emerging\\ngenerative AI opportunities and challenges in the construction industry; (2) to\\npropose a framework for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construction firms to build customized generative AI\\nsolutions using their own data, comprising steps such as data</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collection,\\ndataset curation, training custom large language model (LLM), model evaluation,\\nand deployment; and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(3) to demonstrate the framework via a case study of\\ndeveloping a generative model for querying contract </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents. The results show\\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\\n9.4, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">4.8% in terms of quality, relevance, and reproducibility. This study\\nprovides academics and construction </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">professionals with a comprehensive analysis\\nand practical framework to guide the adoption of generative AI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">techniques to\\nenhance productivity, quality, safety, and sustainability across the\\nconstruction industry.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-02-15'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Generative AI in the Construction Industry: A State-of-the-art Analysis'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun\u001b[0m\n",
       "\u001b[32mSalami, Abdullahi Saka, Tarek Zayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The construction industry is a vital sector of the global economy, but it\\nfaces many productivity \u001b[0m\n",
       "\u001b[32mchallenges in various processes, such as design,\\nplanning, procurement, inspection, and maintenance. Generative \u001b[0m\n",
       "\u001b[32martificial\\nintelligence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which can create novel and realistic data or content, such\\nas text, image, video, \u001b[0m\n",
       "\u001b[32mor code, based on some input or prior knowledge, offers\\ninnovative and disruptive solutions to address these \u001b[0m\n",
       "\u001b[32mchallenges. However, there\\nis a gap in the literature on the current state, opportunities, and challenges\\nof \u001b[0m\n",
       "\u001b[32mgenerative AI in the construction industry. This study aims to fill this gap\\nby providing a state-of-the-art \u001b[0m\n",
       "\u001b[32manalysis of generative AI in construction, with\\nthree objectives: \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to review and categorize the existing and \u001b[0m\n",
       "\u001b[32memerging\\ngenerative AI opportunities and challenges in the construction industry; \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to\\npropose a framework for \u001b[0m\n",
       "\u001b[32mconstruction firms to build customized generative AI\\nsolutions using their own data, comprising steps such as data\u001b[0m\n",
       "\u001b[32mcollection,\\ndataset curation, training custom large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, model evaluation,\\nand deployment; and \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to demonstrate the framework via a case study of\\ndeveloping a generative model for querying contract \u001b[0m\n",
       "\u001b[32mdocuments. The results show\\nthat retrieval augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m improves the baseline LLM by 5.2,\\n9.4, and \u001b[0m\n",
       "\u001b[32m4.8% in terms of quality, relevance, and reproducibility. This study\\nprovides academics and construction \u001b[0m\n",
       "\u001b[32mprofessionals with a comprehensive analysis\\nand practical framework to guide the adoption of generative AI \u001b[0m\n",
       "\u001b[32mtechniques to\\nenhance productivity, quality, safety, and sustainability across the\\nconstruction industry.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 10\n",
      " - # Chunks: 70\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-07-23'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Zhibo Hu, Chen Wang, Yanfeng Shu, Helen, Paik, Liming Zhu'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"The robustness of large language models (LLMs) becomes increasingly important\\nas their use rapidly</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">grows in a wide range of domains. Retrieval-Augmented\\nGeneration (RAG) is considered as a means to improve the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trustworthiness of\\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\\naffected by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slightly different inputs is not well studied. In this work, we\\nfind that the insertion of even a short prefix to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the prompt leads to the\\ngeneration of outputs far away from factually correct answers. We\\nsystematically evaluate</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the effect of such prefixes on RAG by introducing a\\nnovel optimization technique called Gradient Guided Prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Perturbation (GGPP).\\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\\ntargeted wrong </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answers. It can also cope with instructions in the prompts\\nrequesting to ignore irrelevant context. We also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploit LLMs' neuron\\nactivation difference between prompts with and without GGPP perturbations to\\ngive a method </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that improves the robustness of RAG-based LLMs through a highly\\neffective detector trained on neuron activation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">triggered by GGPP generated\\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\\nour </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methods.\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-07-23'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Zhibo Hu, Chen Wang, Yanfeng Shu, Helen, Paik, Liming Zhu'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"The robustness of large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m becomes increasingly important\\nas their use rapidly\u001b[0m\n",
       "\u001b[32mgrows in a wide range of domains. Retrieval-Augmented\\nGeneration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is considered as a means to improve the \u001b[0m\n",
       "\u001b[32mtrustworthiness of\\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\\naffected by \u001b[0m\n",
       "\u001b[32mslightly different inputs is not well studied. In this work, we\\nfind that the insertion of even a short prefix to \u001b[0m\n",
       "\u001b[32mthe prompt leads to the\\ngeneration of outputs far away from factually correct answers. We\\nsystematically evaluate\u001b[0m\n",
       "\u001b[32mthe effect of such prefixes on RAG by introducing a\\nnovel optimization technique called Gradient Guided Prompt \u001b[0m\n",
       "\u001b[32mPerturbation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGGPP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\\ntargeted wrong \u001b[0m\n",
       "\u001b[32manswers. It can also cope with instructions in the prompts\\nrequesting to ignore irrelevant context. We also \u001b[0m\n",
       "\u001b[32mexploit LLMs' neuron\\nactivation difference between prompts with and without GGPP perturbations to\\ngive a method \u001b[0m\n",
       "\u001b[32mthat improves the robustness of RAG-based LLMs through a highly\\neffective detector trained on neuron activation \u001b[0m\n",
       "\u001b[32mtriggered by GGPP generated\\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\\nour \u001b[0m\n",
       "\u001b[32mmethods.\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 11\n",
      " - # Chunks: 51\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-02-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhoujun Li, Xi Zhu, Chengwei Pan'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The integration of multimodal Electronic Health Records (EHR) data has\\nsignificantly improved </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">clinical predictive capabilities. Leveraging clinical\\nnotes and multivariate time-series EHR, existing models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">often lack the medical\\ncontext relevent to clinical tasks, prompting the incorporation of external\\nknowledge, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">particularly from the knowledge graph (KG). Previous approaches with\\nKG knowledge have primarily focused on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structured knowledge extraction,\\nneglecting unstructured data modalities and semantic high dimensional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">medical\\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\\n(RAG) driven framework to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enhance multimodal EHR representations that address\\nthese limitations. Firstly, we apply Large Language Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(LLM) to encode long\\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\\nwe prompt LLM</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to extract task-relevant medical entities and match entities in\\nprofessionally labeled external knowledge graph </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(PrimeKG) with corresponding\\nmedical knowledge. By matching and aligning with clinical standards, our\\nframework </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eliminates hallucinations and ensures consistency. Lastly, we propose\\nan adaptive multimodal fusion network to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrate extracted knowledge with\\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nreadmission tasks showcase the superior performance of our REALM framework over\\nbaselines, emphasizing the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effectiveness of each module. REALM framework\\ncontributes to refining the use of multimodal EHR data in healthcare</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nbridging the gap with nuanced medical context essential for informed clinical\\npredictions.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-02-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language \u001b[0m\n",
       "\u001b[32mModels'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, \u001b[0m\n",
       "\u001b[32mZhoujun Li, Xi Zhu, Chengwei Pan'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The integration of multimodal Electronic Health Records \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEHR\u001b[0m\u001b[32m)\u001b[0m\u001b[32m data has\\nsignificantly improved \u001b[0m\n",
       "\u001b[32mclinical predictive capabilities. Leveraging clinical\\nnotes and multivariate time-series EHR, existing models \u001b[0m\n",
       "\u001b[32moften lack the medical\\ncontext relevent to clinical tasks, prompting the incorporation of external\\nknowledge, \u001b[0m\n",
       "\u001b[32mparticularly from the knowledge graph \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Previous approaches with\\nKG knowledge have primarily focused on \u001b[0m\n",
       "\u001b[32mstructured knowledge extraction,\\nneglecting unstructured data modalities and semantic high dimensional \u001b[0m\n",
       "\u001b[32mmedical\\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m driven framework to \u001b[0m\n",
       "\u001b[32menhance multimodal EHR representations that address\\nthese limitations. Firstly, we apply Large Language Model \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to encode long\\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\\nwe prompt LLM\u001b[0m\n",
       "\u001b[32mto extract task-relevant medical entities and match entities in\\nprofessionally labeled external knowledge graph \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mPrimeKG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with corresponding\\nmedical knowledge. By matching and aligning with clinical standards, our\\nframework \u001b[0m\n",
       "\u001b[32meliminates hallucinations and ensures consistency. Lastly, we propose\\nan adaptive multimodal fusion network to \u001b[0m\n",
       "\u001b[32mintegrate extracted knowledge with\\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality \u001b[0m\n",
       "\u001b[32mand\\nreadmission tasks showcase the superior performance of our REALM framework over\\nbaselines, emphasizing the \u001b[0m\n",
       "\u001b[32meffectiveness of each module. REALM framework\\ncontributes to refining the use of multimodal EHR data in healthcare\u001b[0m\n",
       "\u001b[32mand\\nbridging the gap with nuanced medical context essential for informed clinical\\npredictions.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 12\n",
      " - # Chunks: 43\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-09-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-augmented generation (RAG) has rapidly advanced the language model\\nfield, particularly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in question-answering (QA) systems. By integrating external\\ndocuments during the response generation phase, RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly enhances the\\naccuracy and reliability of language models. This method elevates the quality\\nof </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses and reduces the frequency of hallucinations, where the model\\ngenerates incorrect or misleading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information. However, these methods exhibit\\nlimited retrieval accuracy when faced with numerous </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">indistinguishable\\ndocuments, presenting notable challenges in their practical application. In\\nresponse to these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">emerging challenges, we present HiQA, an advanced\\nmulti-document question-answering (MDQA) framework that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrates cascading\\nmetadata into content and a multi-route retrieval mechanism. We also release a\\nbenchmark </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">called MasQA to evaluate and research in MDQA. Finally, HiQA\\ndemonstrates the state-of-the-art performance in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-document environments.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-09-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has rapidly advanced the language model\\nfield, particularly \u001b[0m\n",
       "\u001b[32min question-answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m systems. By integrating external\\ndocuments during the response generation phase, RAG \u001b[0m\n",
       "\u001b[32msignificantly enhances the\\naccuracy and reliability of language models. This method elevates the quality\\nof \u001b[0m\n",
       "\u001b[32mresponses and reduces the frequency of hallucinations, where the model\\ngenerates incorrect or misleading \u001b[0m\n",
       "\u001b[32minformation. However, these methods exhibit\\nlimited retrieval accuracy when faced with numerous \u001b[0m\n",
       "\u001b[32mindistinguishable\\ndocuments, presenting notable challenges in their practical application. In\\nresponse to these \u001b[0m\n",
       "\u001b[32memerging challenges, we present HiQA, an advanced\\nmulti-document question-answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMDQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m framework that \u001b[0m\n",
       "\u001b[32mintegrates cascading\\nmetadata into content and a multi-route retrieval mechanism. We also release a\\nbenchmark \u001b[0m\n",
       "\u001b[32mcalled MasQA to evaluate and research in MDQA. Finally, HiQA\\ndemonstrates the state-of-the-art performance in \u001b[0m\n",
       "\u001b[32mmulti-document environments.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 13\n",
      " - # Chunks: 36\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-03-16'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Financial Report Chunking for Effective Retrieval Augmented Generation'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Renyu Li'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Chunking information is a key step in Retrieval Augmented Generation (RAG).\\nCurrent research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">primarily centers on paragraph-level chunking. This approach\\ntreats all texts as equal and neglects the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information contained in the\\nstructure of documents. We propose an expanded approach to chunk documents by\\nmoving</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">beyond mere paragraph-level chunking to chunk primary by structural\\nelement components of documents. Dissecting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents into these constituent\\nelements creates a new way to chunk documents that yields the best chunk </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">size\\nwithout tuning. We introduce a novel framework that evaluates how chunking\\nbased on element types annotated </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by document understanding models contributes\\nto the overall context and accuracy of the information retrieved. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">also\\ndemonstrate how this approach impacts RAG assisted Question &amp; Answer task\\nperformance. Our research includes</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a comprehensive analysis of various element\\ntypes, their role in effective information retrieval, and the impact </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they have\\non the quality of RAG outputs. Findings support that element type based\\nchunking largely improve RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results on financial reporting. Through this\\nresearch, we are also able to answer how to uncover highly accurate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-03-16'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Financial Report Chunking for Effective Retrieval Augmented Generation'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Renyu Li'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Chunking information is a key step in Retrieval Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nCurrent research \u001b[0m\n",
       "\u001b[32mprimarily centers on paragraph-level chunking. This approach\\ntreats all texts as equal and neglects the \u001b[0m\n",
       "\u001b[32minformation contained in the\\nstructure of documents. We propose an expanded approach to chunk documents by\\nmoving\u001b[0m\n",
       "\u001b[32mbeyond mere paragraph-level chunking to chunk primary by structural\\nelement components of documents. Dissecting \u001b[0m\n",
       "\u001b[32mdocuments into these constituent\\nelements creates a new way to chunk documents that yields the best chunk \u001b[0m\n",
       "\u001b[32msize\\nwithout tuning. We introduce a novel framework that evaluates how chunking\\nbased on element types annotated \u001b[0m\n",
       "\u001b[32mby document understanding models contributes\\nto the overall context and accuracy of the information retrieved. We \u001b[0m\n",
       "\u001b[32malso\\ndemonstrate how this approach impacts RAG assisted Question & Answer task\\nperformance. Our research includes\u001b[0m\n",
       "\u001b[32ma comprehensive analysis of various element\\ntypes, their role in effective information retrieval, and the impact \u001b[0m\n",
       "\u001b[32mthey have\\non the quality of RAG outputs. Findings support that element type based\\nchunking largely improve RAG \u001b[0m\n",
       "\u001b[32mresults on financial reporting. Through this\\nresearch, we are also able to answer how to uncover highly accurate \u001b[0m\n",
       "\u001b[32mRAG.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 14\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'AI4Contracts: LLM &amp; RAG-Powered Encoding of Financial Derivative Contracts'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Maruf Ahmed Mridul, Ian Sloyan, Aparna Gupta, Oshani Seneviratne'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) are\\nreshaping how AI systems</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extract and organize information from unstructured\\ntext. A key challenge is designing AI methods that can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">incrementally extract,\\nstructure, and validate information while preserving hierarchical and\\ncontextual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relationships. We introduce CDMizer, a template-driven, LLM, and\\nRAG-based framework for structured text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transformation. By leveraging\\ndepth-based retrieval and hierarchical generation, CDMizer ensures a\\ncontrolled, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modular process that aligns generated outputs with predefined\\nschema. Its template-driven approach guarantees </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">syntactic correctness, schema\\nadherence, and improved scalability, addressing key limitations of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">direct\\ngeneration methods. Additionally, we propose an LLM-powered evaluation\\nframework to assess the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">completeness and accuracy of structured\\nrepresentations. Demonstrated in the transformation of Over-the-Counter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(OTC)\\nfinancial derivative contracts into the Common Domain Model (CDM), CDMizer\\nestablishes a scalable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">foundation for AI-driven document understanding,\\nstructured synthesis, and automated validation in broader </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contexts.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-06-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative Contracts'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Maruf Ahmed Mridul, Ian Sloyan, Aparna Gupta, Oshani Seneviratne'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Retrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are\\nreshaping how AI systems\u001b[0m\n",
       "\u001b[32mextract and organize information from unstructured\\ntext. A key challenge is designing AI methods that can \u001b[0m\n",
       "\u001b[32mincrementally extract,\\nstructure, and validate information while preserving hierarchical and\\ncontextual \u001b[0m\n",
       "\u001b[32mrelationships. We introduce CDMizer, a template-driven, LLM, and\\nRAG-based framework for structured text \u001b[0m\n",
       "\u001b[32mtransformation. By leveraging\\ndepth-based retrieval and hierarchical generation, CDMizer ensures a\\ncontrolled, \u001b[0m\n",
       "\u001b[32mmodular process that aligns generated outputs with predefined\\nschema. Its template-driven approach guarantees \u001b[0m\n",
       "\u001b[32msyntactic correctness, schema\\nadherence, and improved scalability, addressing key limitations of \u001b[0m\n",
       "\u001b[32mdirect\\ngeneration methods. Additionally, we propose an LLM-powered evaluation\\nframework to assess the \u001b[0m\n",
       "\u001b[32mcompleteness and accuracy of structured\\nrepresentations. Demonstrated in the transformation of Over-the-Counter \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mOTC\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nfinancial derivative contracts into the Common Domain Model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCDM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, CDMizer\\nestablishes a scalable \u001b[0m\n",
       "\u001b[32mfoundation for AI-driven document understanding,\\nstructured synthesis, and automated validation in broader \u001b[0m\n",
       "\u001b[32mcontexts.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 15\n",
      " - # Chunks: 49\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-05-16'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wang, Yun Ma'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various\\nRAG paradigms, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including vanilla, planning-based, and iterative RAG, are built\\nupon 2 cores: the retriever, which should robustly</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">select relevant documents\\nacross complex queries, and the generator, which should faithfully </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesize\\nresponses. However, existing retrievers rely heavily on public knowledge and\\nstruggle with queries of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">varying logical complexity and clue completeness,\\nwhile generators frequently face fidelity problems. In this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">work, we introduce\\nRAGSynth, a framework that includes a data construction modeling and a\\ncorresponding synthetic</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data generation implementation, designed to optimize\\nretriever robustness and generator fidelity. Additionally, we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">present\\nSynthBench, a benchmark encompassing 8 domain-specific documents across 4\\ndomains, featuring diverse </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query complexities, clue completeness, and\\nfine-grained citation granularity. Leveraging RAGSynth, we generate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a\\nlarge-scale synthetic dataset, including single and multi-hop. Extensive\\nexperiments demonstrate that the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthetic data significantly improves the\\nrobustness of the retrievers and the fidelity of the generators. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Additional\\nevaluations confirm that RAGSynth can also generalize well across different\\ndomains. By integrating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the optimized retrievers into various RAG paradigms, we\\nconsistently observe enhanced RAG system performance. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have open-sourced the\\nimplementation on https://github.com/EachSheep/RAGSynth.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-05-16'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng \u001b[0m\n",
       "\u001b[32mWang, Yun Ma'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various\\nRAG paradigms, \u001b[0m\n",
       "\u001b[32mincluding vanilla, planning-based, and iterative RAG, are built\\nupon 2 cores: the retriever, which should robustly\u001b[0m\n",
       "\u001b[32mselect relevant documents\\nacross complex queries, and the generator, which should faithfully \u001b[0m\n",
       "\u001b[32msynthesize\\nresponses. However, existing retrievers rely heavily on public knowledge and\\nstruggle with queries of \u001b[0m\n",
       "\u001b[32mvarying logical complexity and clue completeness,\\nwhile generators frequently face fidelity problems. In this \u001b[0m\n",
       "\u001b[32mwork, we introduce\\nRAGSynth, a framework that includes a data construction modeling and a\\ncorresponding synthetic\u001b[0m\n",
       "\u001b[32mdata generation implementation, designed to optimize\\nretriever robustness and generator fidelity. Additionally, we\u001b[0m\n",
       "\u001b[32mpresent\\nSynthBench, a benchmark encompassing 8 domain-specific documents across 4\\ndomains, featuring diverse \u001b[0m\n",
       "\u001b[32mquery complexities, clue completeness, and\\nfine-grained citation granularity. Leveraging RAGSynth, we generate \u001b[0m\n",
       "\u001b[32ma\\nlarge-scale synthetic dataset, including single and multi-hop. Extensive\\nexperiments demonstrate that the \u001b[0m\n",
       "\u001b[32msynthetic data significantly improves the\\nrobustness of the retrievers and the fidelity of the generators. \u001b[0m\n",
       "\u001b[32mAdditional\\nevaluations confirm that RAGSynth can also generalize well across different\\ndomains. By integrating \u001b[0m\n",
       "\u001b[32mthe optimized retrievers into various RAG paradigms, we\\nconsistently observe enhanced RAG system performance. We \u001b[0m\n",
       "\u001b[32mhave open-sourced the\\nimplementation on https://github.com/EachSheep/RAGSynth.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ## Some longer papers\n",
    "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "    ArxivLoader(query=\"2312.10997\").load(),  ## RAG for LLM\n",
    "    ArxivLoader(query=\"2402.07867\").load(),  ## Knowledge Poisoning Attacks to RAG\n",
    "    ArxivLoader(query=\"2402.08416\").load(),  ## Jailbreak attack RAG\n",
    "    ArxivLoader(query=\"2402.09939\").load(), ## RAG in Construction\n",
    "    ArxivLoader(query=\"2402.07179\").load(), ## Prompt Perturbation in RAG\n",
    "    ArxivLoader(query=\"2402.07016\").load(), ## RAG in Health Records\n",
    "    ArxivLoader(query=\"2402.01767\").load(), ## Hierarchical Contextual RAG\n",
    "    ArxivLoader(query=\"2402.05131\").load(), ## Financial Report RAG\n",
    "    ArxivLoader(query=\"2506.01063\").load(), ## LLM & RAG-Powered Encoding of Financial Derivative Contracts\n",
    "    ArxivLoader(query=\"2505.10989\").load(), ## RAGSynth\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2**: Construct Your Document Vector Stores\n",
    "\n",
    "Now that we have all of the components, we can go ahead and create indices surrounding them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 2.2 s, sys: 140 ms, total: 2.34 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "From there, we can combine our indices into a single one using the following utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 918 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 3: [Exercise]** Implement Your RAG Chain\n",
    "\n",
    "Finally, all the puzzle pieces are in place to implement the RAG pipeline! As a review, we now have:\n",
    "\n",
    "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
    "\n",
    "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n",
    "\n",
    "With the help of a couple more utilities, you're finally ready to integrate your chain! A few additional convenience utilities are provided (`doc2str` and the now-common `RPrint`) but are optional to use. Additionally, some starter prompts and structures are also defined.\n",
    "\n",
    "> **Given all of this:** Please implement the `retrieval_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Language Models] . As shown in Figure 1, there are three\\\\ncomponents in RAG: knowledge database, retriever, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM.\\\\nA knowledge database contains a large number of texts col-\\\\nlected from various sources such as Wikipedia </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[17], finan-\\\\ncial documents [7], news articles [18], COVID-19 publica-\\\\ntions [19], to name a few. A retriever </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is used to retrieve a\\\\nset of most relevant texts from the knowledge database for a\\\\nquestion. With the help of a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system prompt, the retrieved texts\\\\nare used as the context for the LLM to generate an answer for\\\\nthe given </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question. RAG enables an LLM to utilize external\\\\nknowledge in a plug-and-play manner. Moreover, RAG can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">re-\\\\nduce hallucinations and enhance the domain-specific expertise\\\\nof an LLM. Due to these benefits, we have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">witnessed a vari-\\\\n1\\\\narXiv:2402.07867v3  [cs.CR]  13 Aug 2024\\\\nety of developed tools (e.g., ChatGPT Retrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Plugin [20],\\\\nLlamaIndex [21], ChatRTX [22], and LangChain [23]) and\\\\nreal-world applications (e.g\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Large Language Models: A Survey] the technology tree summarizing related </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">research is shown\\nCorresponding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Surve</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">y\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characteristics.\\nInitially, RAG‚Äôs inception coincided with the rise of the\\nTransformer architecture, focusing on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]‚Äì[5].The subsequent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">research\\n[Quote from Financial Report Chunking for Effective Retrieval Augmented Generation] . In RAG, instead of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answering a user\\\\nquery directly using an LLM, the user query is used to retrieve documents or\\\\nsegments from a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus and the top retrieved documents or segments are used\\\\nto generate the answer in conjunction with an LLM. In</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this way, RAG con-\\\\nstraints the answer to the set of retrieved documents. RAGs have been used as\\\\nwell to answer</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions from single documents [14]. The documents are split\\\\ninto smaller parts or chunks, indexed by a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval system and recovered and\\\\nprocessed depending on the user information need. In a sense, this process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allows\\\\nanswering questions about information in a single document, thus contributing\\\\nto the set of techniques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">available for document understanding.\\\\nSince documents need to be chunked for RAG processing, this raises </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\\\nquestion about what is the best practice to chunk documents for e\\\\ufb00ective RAG\\\\ndocument </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">understanding\\n[Quote from Retrieval-Augmented Generation for Large Language Models: A Survey] on RAG‚Äôs downstream </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">discussed news. Given ChatGPT‚Äôs reliance on pre-\\ntraining data, it initially lacks the capacity to provide </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user‚Äôs query. These articles, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\n'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m''\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large \u001b[0m\n",
       "\u001b[32mLanguage Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . As shown in Figure 1, there are three\\\\ncomponents in RAG: knowledge database, retriever, and \u001b[0m\n",
       "\u001b[32mLLM.\\\\nA knowledge database contains a large number of texts col-\\\\nlected from various sources such as Wikipedia \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m17\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, finan-\\\\ncial documents \u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, news articles \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, COVID-19 publica-\\\\ntions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, to name a few. A retriever \u001b[0m\n",
       "\u001b[32mis used to retrieve a\\\\nset of most relevant texts from the knowledge database for a\\\\nquestion. With the help of a\u001b[0m\n",
       "\u001b[32msystem prompt, the retrieved texts\\\\nare used as the context for the LLM to generate an answer for\\\\nthe given \u001b[0m\n",
       "\u001b[32mquestion. RAG enables an LLM to utilize external\\\\nknowledge in a plug-and-play manner. Moreover, RAG can \u001b[0m\n",
       "\u001b[32mre-\\\\nduce hallucinations and enhance the domain-specific expertise\\\\nof an LLM. Due to these benefits, we have \u001b[0m\n",
       "\u001b[32mwitnessed a vari-\\\\n1\\\\narXiv:2402.07867v3  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CR\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  13 Aug 2024\\\\nety of developed tools \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., ChatGPT Retrieval \u001b[0m\n",
       "\u001b[32mPlugin \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\\nLlamaIndex \u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, ChatRTX \u001b[0m\u001b[32m[\u001b[0m\u001b[32m22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and LangChain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m23\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\\\\nreal-world applications \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\u001b[32m]\u001b[0m\u001b[32m the technology tree summarizing related \u001b[0m\n",
       "\u001b[32mresearch is shown\\nCorresponding \u001b[0m\n",
       "\u001b[32mAuthor.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Surve\u001b[0m\n",
       "\u001b[32my\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage \u001b[0m\n",
       "\u001b[32mcharacteristics.\\nInitially, RAG‚Äôs inception coincided with the rise of the\\nTransformer architecture, focusing on \u001b[0m\n",
       "\u001b[32menhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPTM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This early \u001b[0m\n",
       "\u001b[32mstage was characterized\\nby foundational work aimed at refining pre-training techniques\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m‚Äì\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.The subsequent \u001b[0m\n",
       "\u001b[32marrival of ChatGPT \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mcapabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and \u001b[0m\n",
       "\u001b[32mknowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As \u001b[0m\n",
       "\u001b[32mresearch\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Financial Report Chunking for Effective Retrieval Augmented Generation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In RAG, instead of \u001b[0m\n",
       "\u001b[32manswering a user\\\\nquery directly using an LLM, the user query is used to retrieve documents or\\\\nsegments from a \u001b[0m\n",
       "\u001b[32mcorpus and the top retrieved documents or segments are used\\\\nto generate the answer in conjunction with an LLM. In\u001b[0m\n",
       "\u001b[32mthis way, RAG con-\\\\nstraints the answer to the set of retrieved documents. RAGs have been used as\\\\nwell to answer\u001b[0m\n",
       "\u001b[32mquestions from single documents \u001b[0m\u001b[32m[\u001b[0m\u001b[32m14\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. The documents are split\\\\ninto smaller parts or chunks, indexed by a \u001b[0m\n",
       "\u001b[32mretrieval system and recovered and\\\\nprocessed depending on the user information need. In a sense, this process \u001b[0m\n",
       "\u001b[32mallows\\\\nanswering questions about information in a single document, thus contributing\\\\nto the set of techniques \u001b[0m\n",
       "\u001b[32mavailable for document understanding.\\\\nSince documents need to be chunked for RAG processing, this raises \u001b[0m\n",
       "\u001b[32mthe\\\\nquestion about what is the best practice to chunk documents for e\\\\ufb00ective RAG\\\\ndocument \u001b[0m\n",
       "\u001b[32munderstanding\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\u001b[32m]\u001b[0m\u001b[32m on RAG‚Äôs downstream \u001b[0m\n",
       "\u001b[32mtasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its \u001b[0m\n",
       "\u001b[32mfuture development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical \u001b[0m\n",
       "\u001b[32mapplication of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely \u001b[0m\n",
       "\u001b[32mdiscussed news. Given ChatGPT‚Äôs reliance on pre-\\ntraining data, it initially lacks the capacity to provide \u001b[0m\n",
       "\u001b[32mup-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from \u001b[0m\n",
       "\u001b[32mexternal\\ndatabases. In this case, it gathers relevant news articles related\\nto the user‚Äôs query. These articles, \u001b[0m\n",
       "\u001b[32mcombined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed \u001b[0m\n",
       "\u001b[32manswer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, \u001b[0m\n",
       "\u001b[32mAdvanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\n'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from PoisonedRAG: Knowledge Corruption Attacks to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation of Large Language Models] . As shown in Figure 1, there are three\\\\ncomponents in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG: knowledge database, retriever, and LLM.\\\\nA knowledge database contains a large number of texts col-\\\\nlected </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">from various sources such as Wikipedia [17], finan-\\\\ncial documents [7], news articles [18], COVID-19 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">publica-\\\\ntions [19], to name a few. A retriever is used to retrieve a\\\\nset of most relevant texts from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge database for a\\\\nquestion. With the help of a system prompt, the retrieved texts\\\\nare used as the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context for the LLM to generate an answer for\\\\nthe given question. RAG enables an LLM to utilize </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">external\\\\nknowledge in a plug-and-play manner. Moreover, RAG can re-\\\\nduce hallucinations and enhance the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">domain-specific expertise\\\\nof an LLM. Due to these benefits, we have witnessed a vari-\\\\n1\\\\narXiv:2402.07867v3  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[cs.CR]  13 Aug 2024\\\\nety of developed tools (e.g., ChatGPT Retrieval Plugin [20],\\\\nLlamaIndex [21], ChatRTX </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[22], and LangChain [23]) and\\\\nreal-world applications (e.g\\n[Quote from Retrieval-Augmented Generation for Large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Language Models: A Survey] the technology tree summarizing related research is shown\\nCorresponding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Surve</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">y\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characteristics.\\nInitially, RAG‚Äôs inception coincided with the rise of the\\nTransformer architecture, focusing on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]‚Äì[5].The subsequent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">research\\n[Quote from Financial Report Chunking for Effective Retrieval Augmented Generation] . In RAG, instead of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answering a user\\\\nquery directly using an LLM, the user query is used to retrieve documents or\\\\nsegments from a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus and the top retrieved documents or segments are used\\\\nto generate the answer in conjunction with an LLM. In</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this way, RAG con-\\\\nstraints the answer to the set of retrieved documents. RAGs have been used as\\\\nwell to answer</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions from single documents [14]. The documents are split\\\\ninto smaller parts or chunks, indexed by a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval system and recovered and\\\\nprocessed depending on the user information need. In a sense, this process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allows\\\\nanswering questions about information in a single document, thus contributing\\\\nto the set of techniques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">available for document understanding.\\\\nSince documents need to be chunked for RAG processing, this raises </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\\\nquestion about what is the best practice to chunk documents for e\\\\ufb00ective RAG\\\\ndocument </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">understanding\\n[Quote from Retrieval-Augmented Generation for Large Language Models: A Survey] on RAG‚Äôs downstream </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">discussed news. Given ChatGPT‚Äôs reliance on pre-\\ntraining data, it initially lacks the capacity to provide </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user‚Äôs query. These articles, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\n\\n\\n (Answer only from retrieval. Only cite sources</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that are used. Make your response conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
       "\u001b[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from PoisonedRAG: Knowledge Corruption Attacks to\u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation of Large Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . As shown in Figure 1, there are three\\\\ncomponents in \u001b[0m\n",
       "\u001b[32mRAG: knowledge database, retriever, and LLM.\\\\nA knowledge database contains a large number of texts col-\\\\nlected \u001b[0m\n",
       "\u001b[32mfrom various sources such as Wikipedia \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, finan-\\\\ncial documents \u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, news articles \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, COVID-19 \u001b[0m\n",
       "\u001b[32mpublica-\\\\ntions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, to name a few. A retriever is used to retrieve a\\\\nset of most relevant texts from the \u001b[0m\n",
       "\u001b[32mknowledge database for a\\\\nquestion. With the help of a system prompt, the retrieved texts\\\\nare used as the \u001b[0m\n",
       "\u001b[32mcontext for the LLM to generate an answer for\\\\nthe given question. RAG enables an LLM to utilize \u001b[0m\n",
       "\u001b[32mexternal\\\\nknowledge in a plug-and-play manner. Moreover, RAG can re-\\\\nduce hallucinations and enhance the \u001b[0m\n",
       "\u001b[32mdomain-specific expertise\\\\nof an LLM. Due to these benefits, we have witnessed a vari-\\\\n1\\\\narXiv:2402.07867v3  \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mcs.CR\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  13 Aug 2024\\\\nety of developed tools \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., ChatGPT Retrieval Plugin \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\\nLlamaIndex \u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, ChatRTX \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and LangChain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m23\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\\\\nreal-world applications \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Large \u001b[0m\n",
       "\u001b[32mLanguage Models: A Survey\u001b[0m\u001b[32m]\u001b[0m\u001b[32m the technology tree summarizing related research is shown\\nCorresponding \u001b[0m\n",
       "\u001b[32mAuthor.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Surve\u001b[0m\n",
       "\u001b[32my\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage \u001b[0m\n",
       "\u001b[32mcharacteristics.\\nInitially, RAG‚Äôs inception coincided with the rise of the\\nTransformer architecture, focusing on \u001b[0m\n",
       "\u001b[32menhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPTM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This early \u001b[0m\n",
       "\u001b[32mstage was characterized\\nby foundational work aimed at refining pre-training techniques\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m‚Äì\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.The subsequent \u001b[0m\n",
       "\u001b[32marrival of ChatGPT \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mcapabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and \u001b[0m\n",
       "\u001b[32mknowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As \u001b[0m\n",
       "\u001b[32mresearch\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Financial Report Chunking for Effective Retrieval Augmented Generation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In RAG, instead of \u001b[0m\n",
       "\u001b[32manswering a user\\\\nquery directly using an LLM, the user query is used to retrieve documents or\\\\nsegments from a \u001b[0m\n",
       "\u001b[32mcorpus and the top retrieved documents or segments are used\\\\nto generate the answer in conjunction with an LLM. In\u001b[0m\n",
       "\u001b[32mthis way, RAG con-\\\\nstraints the answer to the set of retrieved documents. RAGs have been used as\\\\nwell to answer\u001b[0m\n",
       "\u001b[32mquestions from single documents \u001b[0m\u001b[32m[\u001b[0m\u001b[32m14\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. The documents are split\\\\ninto smaller parts or chunks, indexed by a \u001b[0m\n",
       "\u001b[32mretrieval system and recovered and\\\\nprocessed depending on the user information need. In a sense, this process \u001b[0m\n",
       "\u001b[32mallows\\\\nanswering questions about information in a single document, thus contributing\\\\nto the set of techniques \u001b[0m\n",
       "\u001b[32mavailable for document understanding.\\\\nSince documents need to be chunked for RAG processing, this raises \u001b[0m\n",
       "\u001b[32mthe\\\\nquestion about what is the best practice to chunk documents for e\\\\ufb00ective RAG\\\\ndocument \u001b[0m\n",
       "\u001b[32munderstanding\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\u001b[32m]\u001b[0m\u001b[32m on RAG‚Äôs downstream \u001b[0m\n",
       "\u001b[32mtasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its \u001b[0m\n",
       "\u001b[32mfuture development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical \u001b[0m\n",
       "\u001b[32mapplication of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely \u001b[0m\n",
       "\u001b[32mdiscussed news. Given ChatGPT‚Äôs reliance on pre-\\ntraining data, it initially lacks the capacity to provide \u001b[0m\n",
       "\u001b[32mup-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from \u001b[0m\n",
       "\u001b[32mexternal\\ndatabases. In this case, it gathers relevant news articles related\\nto the user‚Äôs query. These articles, \u001b[0m\n",
       "\u001b[32mcombined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed \u001b[0m\n",
       "\u001b[32manswer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, \u001b[0m\n",
       "\u001b[32mAdvanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources\u001b[0m\n",
       "\u001b[32mthat are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a system that combines language models like ChatGPT with external knowledge databases to generate more informed and accurate responses to user queries. It consists of three main components: a knowledge database, a retriever, and a large language model (LLM) (PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models).\n",
      "\n",
      "The knowledge database is a collection of texts gathered from various sources like Wikipedia, financial documents, news articles, and COVID-19 publications. The retriever is used to find the most relevant texts from the knowledge database based on a user's question. These retrieved texts then serve as context for the LLM, helping it generate an answer for the given question.\n",
      "\n",
      "RAG is beneficial as it enables an LLM to utilize external knowledge in a plug-and-play manner while reducing hallucinations and enhancing domain-specific expertise (PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models).\n",
      "\n",
      "In operation, when a user asks a question, RAG sources and incorporates relevant knowledge from external databases. This is demonstrated in a typical RAG application, where a user asks ChatGPT a question about a recent news event. Since ChatGPT relies on pre-training data and might lack updated information, RAG bridges this gap by gathering relevant news articles related to the user's query. These articles, along with the original question, create a comprehensive prompt that empowers LLMs to generate a well-informed answer (Retrieval-Augmented Generation for Large Language Models: A Survey).\n",
      "\n",
      "Moreover, RAG's research paradigm is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. These stages signify the development trajectory of RAG in the era of large models and encompass various distinct stage characteristics (Retrieval-Augmented Generation for Large Language Models: A Survey)."
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str })\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **Task 4:** Interact With Your Gradio Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://e8313a8a2ccc49a7f0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e8313a8a2ccc49a7f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We can conduct  data construction modeling and a corresponding synthetic data generation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementation, designed to optimize retriever robustness and generator fidelity using RAg ?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote from Document] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Agent previously responded with RAG, or Retrieval-Augmented Generation, is a system that combines language models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like ChatGPT with external knowledge databases to generate more informed and accurate responses to user queries. It</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consists of three main components: a knowledge database, a retriever, and a large language model (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models).\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge database is a collection of texts gathered from various sources like Wikipedia, financial documents, news</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">articles, and COVID-19 publications. The retriever is used to find the most relevant texts from the knowledge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">database based on a user's question. These retrieved texts then serve as context for the LLM, helping it generate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an answer for the given question.\\n\\nRAG is beneficial as it enables an LLM to utilize external knowledge in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">plug-and-play manner while reducing hallucinations and enhancing domain-specific expertise (PoisonedRAG: Knowledge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Corruption Attacks to Retrieval-Augmented Generation of Large Language Models).\\n\\nIn operation, when a user asks a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question, RAG sources and incorporates relevant knowledge from external databases. This is demonstrated in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG application, where a user asks ChatGPT a question about a recent news event. Since ChatGPT relies on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training data and might lack updated information, RAG bridges this gap by gathering relevant news articles </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">related to the user's query. These articles, along with the original question, create a comprehensive prompt that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empowers LLMs to generate a well-informed answer (Retrieval-Augmented Generation for Large Language Models: A </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Survey).\\n\\nMoreover, RAG's research paradigm is categorized into three stages: Naive RAG, Advanced RAG, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Modular RAG. These stages signify the development trajectory of RAG in the era of large models and encompass </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">various distinct stage characteristics (Retrieval-Augmented Generation for Large Language Models: A Survey).\\n\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization] . </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Leveraging RAGSYNTH,\\\\nwe generate a large-scale synthetic dataset, including single and multi-hop. Ex-\\\\ntensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments demonstrate that the synthetic data significantly improves the\\\\nrobustness of the retrievers and the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fidelity of the generators. Additional evalua-\\\\ntions confirm that RAGSYNTH can also generalize well across </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different domains.\\\\nBy integrating the optimized retrievers into various RAG paradigms, we consis-\\\\ntently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">observe enhanced RAG system performance. We have open-sourced the\\\\nimplementation on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/EachSheep/RAGSynth.\\\\n1\\\\nIntroduction\\\\nRetrieval-Augmented Generation (RAG) [19, 66, 12] can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enhance the performance of the large\\\\nlanguage models (LLM) on knowledge-intensive tasks. Formally, given user </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">queries Q = {qi}m\\\\ni=1\\\\nand document set D = {Di}N\\\\ni=1, the goal of RAG is to give a answer\\\\n\\\\u02c6\\\\nAi for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each query qi by\\\\nreferencing D\\n[Quote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Optimization] .\\\\n2.2\\\\nData Synthesis Optimization Methods\\\\nThe huge success of scaling law [26] in LLM has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">spurred data synthesis methods. In NLP [8],\\\\nmodel distillation from larger LLMs can be used to enhance the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities of smaller LLMs.\\\\nIn LLM-based agents [43], the integration of LLMs with tools can be optimized using</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthetic\\\\ncode [65] and workflow [47, 63, 61]. In RAG, data synthesis methods encompass strategies for both\\\\nthe</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retriever [35, 28, 67] and the LLM [9, 23, 56, 6]. For the retriever, data synthesis focuses on\\\\ngenerating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query-positive-negative triplets using pure LLMs. Conversely, for the LLM itself, data\\\\nsynthesis aims to improve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the model\\\\u2019s ability to extract answers from retrieved texts\\n[Quote from RAGSynth: Synthetic Data for Robust </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Faithful RAG Component Optimization] . For generators, maintaining fidelity to retrieved content is critical: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when\\\\nfull clues are available, they should synthesize answers with precise citations [20]; when clues </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are\\\\npartial, they should explicitly acknowledge gaps and provide only supported responses; in cases of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">no\\\\nrelevant clues, they should avoid speculative claims [48].\\\\nIn this paper, we propose RAGSYNTH, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">includes an RAG data synthesis modeling to capture\\\\nthe complex mapping relationships among documents, queries, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ground-truth answers, relevant clues,\\\\nand their connections to source documents, along with a specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementation. RAGSYNTH can\\\\nbe used for large-scale RAG data generation for arbitrary domain-specific documents,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supporting\\\\nsingle-hop and multi-hop data, as well as sentence-level clue mapping. Using RAGSYNTH, we\\\\nsynthesize</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a dataset with diverse query logical complexities, clue completeness levels, and hop counts\\\\nto enhance retriever </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">robustness and generator fidelity\\n[Quote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Optimization] . Various\\\\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\\\\nupon 2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cores: the retriever, which should robustly select relevant documents across\\\\ncomplex queries, and the generator, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which should faithfully synthesize responses.\\\\nHowever, existing retrievers rely heavily on public knowledge and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">struggle with\\\\nqueries of varying logical complexity and clue completeness, while generators\\\\nfrequently face </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fidelity problems. In this work, we introduce RAGSYNTH, a frame-\\\\nwork that includes a data construction modeling </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and a corresponding synthetic data\\\\ngeneration implementation, designed to optimize retriever robustness and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generator\\\\nfidelity. Additionally, we present SYNTHBENCH, a benchmark encompassing 8\\\\ndomain-specific documents </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across 4 domains, featuring diverse query complexities,\\\\nclue completeness, and fine-grained citation granularity.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Leveraging RAGSYNTH,\\\\nwe generate a large-scale synthetic dataset, including single and multi-hop\\n'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We can conduct  data construction modeling and a corresponding synthetic data generation \u001b[0m\n",
       "\u001b[32mimplementation, designed to optimize retriever robustness and generator fidelity using RAg ?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with Tell me about RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mAgent previously responded with RAG, or Retrieval-Augmented Generation, is a system that combines language models \u001b[0m\n",
       "\u001b[32mlike ChatGPT with external knowledge databases to generate more informed and accurate responses to user queries. It\u001b[0m\n",
       "\u001b[32mconsists of three main components: a knowledge database, a retriever, and a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mPoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nThe \u001b[0m\n",
       "\u001b[32mknowledge database is a collection of texts gathered from various sources like Wikipedia, financial documents, news\u001b[0m\n",
       "\u001b[32marticles, and COVID-19 publications. The retriever is used to find the most relevant texts from the knowledge \u001b[0m\n",
       "\u001b[32mdatabase based on a user's question. These retrieved texts then serve as context for the LLM, helping it generate \u001b[0m\n",
       "\u001b[32man answer for the given question.\\n\\nRAG is beneficial as it enables an LLM to utilize external knowledge in a \u001b[0m\n",
       "\u001b[32mplug-and-play manner while reducing hallucinations and enhancing domain-specific expertise \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPoisonedRAG: Knowledge \u001b[0m\n",
       "\u001b[32mCorruption Attacks to Retrieval-Augmented Generation of Large Language Models\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nIn operation, when a user asks a\u001b[0m\n",
       "\u001b[32mquestion, RAG sources and incorporates relevant knowledge from external databases. This is demonstrated in a \u001b[0m\n",
       "\u001b[32mtypical RAG application, where a user asks ChatGPT a question about a recent news event. Since ChatGPT relies on \u001b[0m\n",
       "\u001b[32mpre-training data and might lack updated information, RAG bridges this gap by gathering relevant news articles \u001b[0m\n",
       "\u001b[32mrelated to the user's query. These articles, along with the original question, create a comprehensive prompt that \u001b[0m\n",
       "\u001b[32mempowers LLMs to generate a well-informed answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRetrieval-Augmented Generation for Large Language Models: A \u001b[0m\n",
       "\u001b[32mSurvey\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nMoreover, RAG's research paradigm is categorized into three stages: Naive RAG, Advanced RAG, and \u001b[0m\n",
       "\u001b[32mModular RAG. These stages signify the development trajectory of RAG in the era of large models and encompass \u001b[0m\n",
       "\u001b[32mvarious distinct stage characteristics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRetrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . \u001b[0m\n",
       "\u001b[32mLeveraging RAGSYNTH,\\\\nwe generate a large-scale synthetic dataset, including single and multi-hop. Ex-\\\\ntensive \u001b[0m\n",
       "\u001b[32mexperiments demonstrate that the synthetic data significantly improves the\\\\nrobustness of the retrievers and the \u001b[0m\n",
       "\u001b[32mfidelity of the generators. Additional evalua-\\\\ntions confirm that RAGSYNTH can also generalize well across \u001b[0m\n",
       "\u001b[32mdifferent domains.\\\\nBy integrating the optimized retrievers into various RAG paradigms, we consis-\\\\ntently \u001b[0m\n",
       "\u001b[32mobserve enhanced RAG system performance. We have open-sourced the\\\\nimplementation on \u001b[0m\n",
       "\u001b[32mhttps://github.com/EachSheep/RAGSynth.\\\\n1\\\\nIntroduction\\\\nRetrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m19, 66, 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m can \u001b[0m\n",
       "\u001b[32menhance the performance of the large\\\\nlanguage models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on knowledge-intensive tasks. Formally, given user \u001b[0m\n",
       "\u001b[32mqueries Q = \u001b[0m\u001b[32m{\u001b[0m\u001b[32mqi\u001b[0m\u001b[32m}\u001b[0m\u001b[32mm\\\\\u001b[0m\u001b[32mni\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1\u001b[0m\u001b[32m\\\\nand document set D = \u001b[0m\u001b[32m{\u001b[0m\u001b[32mDi\u001b[0m\u001b[32m}\u001b[0m\u001b[32mN\\\\\u001b[0m\u001b[32mni\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1\u001b[0m\u001b[32m, the goal of RAG is to give a answer\\\\n\\\\u02c6\\\\nAi for \u001b[0m\n",
       "\u001b[32meach query qi by\\\\nreferencing D\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component \u001b[0m\n",
       "\u001b[32mOptimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\n2.2\\\\nData Synthesis Optimization Methods\\\\nThe huge success of scaling law \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m in LLM has \u001b[0m\n",
       "\u001b[32mspurred data synthesis methods. In NLP \u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\\nmodel distillation from larger LLMs can be used to enhance the \u001b[0m\n",
       "\u001b[32mcapabilities of smaller LLMs.\\\\nIn LLM-based agents \u001b[0m\u001b[32m[\u001b[0m\u001b[32m43\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, the integration of LLMs with tools can be optimized using\u001b[0m\n",
       "\u001b[32msynthetic\\\\ncode \u001b[0m\u001b[32m[\u001b[0m\u001b[32m65\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and workflow \u001b[0m\u001b[32m[\u001b[0m\u001b[32m47, 63, 61\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In RAG, data synthesis methods encompass strategies for both\\\\nthe\u001b[0m\n",
       "\u001b[32mretriever \u001b[0m\u001b[32m[\u001b[0m\u001b[32m35, 28, 67\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and the LLM \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9, 23, 56, 6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. For the retriever, data synthesis focuses on\\\\ngenerating \u001b[0m\n",
       "\u001b[32mquery-positive-negative triplets using pure LLMs. Conversely, for the LLM itself, data\\\\nsynthesis aims to improve \u001b[0m\n",
       "\u001b[32mthe model\\\\u2019s ability to extract answers from retrieved texts\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust \u001b[0m\n",
       "\u001b[32mand Faithful RAG Component Optimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . For generators, maintaining fidelity to retrieved content is critical: \u001b[0m\n",
       "\u001b[32mwhen\\\\nfull clues are available, they should synthesize answers with precise citations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m; when clues \u001b[0m\n",
       "\u001b[32mare\\\\npartial, they should explicitly acknowledge gaps and provide only supported responses; in cases of \u001b[0m\n",
       "\u001b[32mno\\\\nrelevant clues, they should avoid speculative claims \u001b[0m\u001b[32m[\u001b[0m\u001b[32m48\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\\\nIn this paper, we propose RAGSYNTH, which \u001b[0m\n",
       "\u001b[32mincludes an RAG data synthesis modeling to capture\\\\nthe complex mapping relationships among documents, queries, \u001b[0m\n",
       "\u001b[32mground-truth answers, relevant clues,\\\\nand their connections to source documents, along with a specific \u001b[0m\n",
       "\u001b[32mimplementation. RAGSYNTH can\\\\nbe used for large-scale RAG data generation for arbitrary domain-specific documents,\u001b[0m\n",
       "\u001b[32msupporting\\\\nsingle-hop and multi-hop data, as well as sentence-level clue mapping. Using RAGSYNTH, we\\\\nsynthesize\u001b[0m\n",
       "\u001b[32ma dataset with diverse query logical complexities, clue completeness levels, and hop counts\\\\nto enhance retriever \u001b[0m\n",
       "\u001b[32mrobustness and generator fidelity\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component \u001b[0m\n",
       "\u001b[32mOptimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Various\\\\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\\\\nupon 2 \u001b[0m\n",
       "\u001b[32mcores: the retriever, which should robustly select relevant documents across\\\\ncomplex queries, and the generator, \u001b[0m\n",
       "\u001b[32mwhich should faithfully synthesize responses.\\\\nHowever, existing retrievers rely heavily on public knowledge and \u001b[0m\n",
       "\u001b[32mstruggle with\\\\nqueries of varying logical complexity and clue completeness, while generators\\\\nfrequently face \u001b[0m\n",
       "\u001b[32mfidelity problems. In this work, we introduce RAGSYNTH, a frame-\\\\nwork that includes a data construction modeling \u001b[0m\n",
       "\u001b[32mand a corresponding synthetic data\\\\ngeneration implementation, designed to optimize retriever robustness and \u001b[0m\n",
       "\u001b[32mgenerator\\\\nfidelity. Additionally, we present SYNTHBENCH, a benchmark encompassing 8\\\\ndomain-specific documents \u001b[0m\n",
       "\u001b[32macross 4 domains, featuring diverse query complexities,\\\\nclue completeness, and fine-grained citation granularity.\u001b[0m\n",
       "\u001b[32mLeveraging RAGSYNTH,\\\\nwe generate a large-scale synthetic dataset, including single and multi-hop\\n'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: We can conduct  data construction modeling and a corresponding synthetic data generation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementation, designed to optimize retriever robustness and generator fidelity using RAg ?\\n\\n From this, we have</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved the following potentially-useful info:  Conversation History Retrieval:\\n[Quote from Document] User </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">previously responded with Tell me about RAG!\\n[Quote from Document] Agent previously responded with RAG, or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation, is a system that combines language models like ChatGPT with external knowledge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">databases to generate more informed and accurate responses to user queries. It consists of three main components: a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge database, a retriever, and a large language model (LLM) (PoisonedRAG: Knowledge Corruption Attacks to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation of Large Language Models).\\n\\nThe knowledge database is a collection of texts </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">gathered from various sources like Wikipedia, financial documents, news articles, and COVID-19 publications. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retriever is used to find the most relevant texts from the knowledge database based on a user's question. These </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved texts then serve as context for the LLM, helping it generate an answer for the given question.\\n\\nRAG is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">beneficial as it enables an LLM to utilize external knowledge in a plug-and-play manner while reducing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hallucinations and enhancing domain-specific expertise (PoisonedRAG: Knowledge Corruption Attacks to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation of Large Language Models).\\n\\nIn operation, when a user asks a question, RAG sources</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and incorporates relevant knowledge from external databases. This is demonstrated in a typical RAG application, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where a user asks ChatGPT a question about a recent news event. Since ChatGPT relies on pre-training data and might</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lack updated information, RAG bridges this gap by gathering relevant news articles related to the user's query. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">These articles, along with the original question, create a comprehensive prompt that empowers LLMs to generate a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">well-informed answer (Retrieval-Augmented Generation for Large Language Models: A Survey).\\n\\nMoreover, RAG's </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">research paradigm is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. These stages signify </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the development trajectory of RAG in the era of large models and encompass various distinct stage characteristics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Retrieval-Augmented Generation for Large Language Models: A Survey).\\n\\n\\n Document Retrieval:\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization] . Leveraging RAGSYNTH,\\\\nwe generate a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large-scale synthetic dataset, including single and multi-hop. Ex-\\\\ntensive experiments demonstrate that the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthetic data significantly improves the\\\\nrobustness of the retrievers and the fidelity of the generators. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Additional evalua-\\\\ntions confirm that RAGSYNTH can also generalize well across different domains.\\\\nBy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrating the optimized retrievers into various RAG paradigms, we consis-\\\\ntently observe enhanced RAG system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance. We have open-sourced the\\\\nimplementation on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/EachSheep/RAGSynth.\\\\n1\\\\nIntroduction\\\\nRetrieval-Augmented Generation (RAG) [19, 66, 12] can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enhance the performance of the large\\\\nlanguage models (LLM) on knowledge-intensive tasks. Formally, given user </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">queries Q = {qi}m\\\\ni=1\\\\nand document set D = {Di}N\\\\ni=1, the goal of RAG is to give a answer\\\\n\\\\u02c6\\\\nAi for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each query qi by\\\\nreferencing D\\n[Quote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Optimization] .\\\\n2.2\\\\nData Synthesis Optimization Methods\\\\nThe huge success of scaling law [26] in LLM has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">spurred data synthesis methods. In NLP [8],\\\\nmodel distillation from larger LLMs can be used to enhance the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities of smaller LLMs.\\\\nIn LLM-based agents [43], the integration of LLMs with tools can be optimized using</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthetic\\\\ncode [65] and workflow [47, 63, 61]. In RAG, data synthesis methods encompass strategies for both\\\\nthe</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retriever [35, 28, 67] and the LLM [9, 23, 56, 6]. For the retriever, data synthesis focuses on\\\\ngenerating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query-positive-negative triplets using pure LLMs. Conversely, for the LLM itself, data\\\\nsynthesis aims to improve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the model\\\\u2019s ability to extract answers from retrieved texts\\n[Quote from RAGSynth: Synthetic Data for Robust </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Faithful RAG Component Optimization] . For generators, maintaining fidelity to retrieved content is critical: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when\\\\nfull clues are available, they should synthesize answers with precise citations [20]; when clues </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are\\\\npartial, they should explicitly acknowledge gaps and provide only supported responses; in cases of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">no\\\\nrelevant clues, they should avoid speculative claims [48].\\\\nIn this paper, we propose RAGSYNTH, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">includes an RAG data synthesis modeling to capture\\\\nthe complex mapping relationships among documents, queries, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ground-truth answers, relevant clues,\\\\nand their connections to source documents, along with a specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementation. RAGSYNTH can\\\\nbe used for large-scale RAG data generation for arbitrary domain-specific documents,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supporting\\\\nsingle-hop and multi-hop data, as well as sentence-level clue mapping. Using RAGSYNTH, we\\\\nsynthesize</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a dataset with diverse query logical complexities, clue completeness levels, and hop counts\\\\nto enhance retriever </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">robustness and generator fidelity\\n[Quote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Optimization] . Various\\\\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\\\\nupon 2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cores: the retriever, which should robustly select relevant documents across\\\\ncomplex queries, and the generator, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which should faithfully synthesize responses.\\\\nHowever, existing retrievers rely heavily on public knowledge and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">struggle with\\\\nqueries of varying logical complexity and clue completeness, while generators\\\\nfrequently face </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fidelity problems. In this work, we introduce RAGSYNTH, a frame-\\\\nwork that includes a data construction modeling </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and a corresponding synthetic data\\\\ngeneration implementation, designed to optimize retriever robustness and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generator\\\\nfidelity. Additionally, we present SYNTHBENCH, a benchmark encompassing 8\\\\ndomain-specific documents </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across 4 domains, featuring diverse query complexities,\\\\nclue completeness, and fine-grained citation granularity.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Leveraging RAGSYNTH,\\\\nwe generate a large-scale synthetic dataset, including single and multi-hop\\n\\n\\n (Answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only from retrieval. Only cite sources that are used. Make your response conversational.)\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'We can conduct  data construction modeling and a corresponding synthetic data generation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementation, designed to optimize retriever robustness and generator fidelity using RAg ?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"You\u001b[0m\u001b[32m are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: We can conduct  data construction modeling and a corresponding synthetic data generation \u001b[0m\n",
       "\u001b[32mimplementation, designed to optimize retriever robustness and generator fidelity using RAg ?\\n\\n From this, we have\u001b[0m\n",
       "\u001b[32mretrieved the following potentially-useful info:  Conversation History Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User \u001b[0m\n",
       "\u001b[32mpreviously responded with Tell me about RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with RAG, or \u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation, is a system that combines language models like ChatGPT with external knowledge \u001b[0m\n",
       "\u001b[32mdatabases to generate more informed and accurate responses to user queries. It consists of three main components: a\u001b[0m\n",
       "\u001b[32mknowledge database, a retriever, and a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPoisonedRAG: Knowledge Corruption Attacks to \u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation of Large Language Models\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nThe knowledge database is a collection of texts \u001b[0m\n",
       "\u001b[32mgathered from various sources like Wikipedia, financial documents, news articles, and COVID-19 publications. The \u001b[0m\n",
       "\u001b[32mretriever is used to find the most relevant texts from the knowledge database based on a user's question. These \u001b[0m\n",
       "\u001b[32mretrieved texts then serve as context for the LLM, helping it generate an answer for the given question.\\n\\nRAG is \u001b[0m\n",
       "\u001b[32mbeneficial as it enables an LLM to utilize external knowledge in a plug-and-play manner while reducing \u001b[0m\n",
       "\u001b[32mhallucinations and enhancing domain-specific expertise \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPoisonedRAG: Knowledge Corruption Attacks to \u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation of Large Language Models\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nIn operation, when a user asks a question, RAG sources\u001b[0m\n",
       "\u001b[32mand incorporates relevant knowledge from external databases. This is demonstrated in a typical RAG application, \u001b[0m\n",
       "\u001b[32mwhere a user asks ChatGPT a question about a recent news event. Since ChatGPT relies on pre-training data and might\u001b[0m\n",
       "\u001b[32mlack updated information, RAG bridges this gap by gathering relevant news articles related to the user's query. \u001b[0m\n",
       "\u001b[32mThese articles, along with the original question, create a comprehensive prompt that empowers LLMs to generate a \u001b[0m\n",
       "\u001b[32mwell-informed answer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRetrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nMoreover, RAG's \u001b[0m\n",
       "\u001b[32mresearch paradigm is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. These stages signify \u001b[0m\n",
       "\u001b[32mthe development trajectory of RAG in the era of large models and encompass various distinct stage characteristics \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mRetrieval-Augmented Generation for Large Language Models: A Survey\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
       "\u001b[32mRAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Leveraging RAGSYNTH,\\\\nwe generate a\u001b[0m\n",
       "\u001b[32mlarge-scale synthetic dataset, including single and multi-hop. Ex-\\\\ntensive experiments demonstrate that the \u001b[0m\n",
       "\u001b[32msynthetic data significantly improves the\\\\nrobustness of the retrievers and the fidelity of the generators. \u001b[0m\n",
       "\u001b[32mAdditional evalua-\\\\ntions confirm that RAGSYNTH can also generalize well across different domains.\\\\nBy \u001b[0m\n",
       "\u001b[32mintegrating the optimized retrievers into various RAG paradigms, we consis-\\\\ntently observe enhanced RAG system \u001b[0m\n",
       "\u001b[32mperformance. We have open-sourced the\\\\nimplementation on \u001b[0m\n",
       "\u001b[32mhttps://github.com/EachSheep/RAGSynth.\\\\n1\\\\nIntroduction\\\\nRetrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m19, 66, 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m can \u001b[0m\n",
       "\u001b[32menhance the performance of the large\\\\nlanguage models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on knowledge-intensive tasks. Formally, given user \u001b[0m\n",
       "\u001b[32mqueries Q = \u001b[0m\u001b[32m{\u001b[0m\u001b[32mqi\u001b[0m\u001b[32m}\u001b[0m\u001b[32mm\\\\\u001b[0m\u001b[32mni\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1\u001b[0m\u001b[32m\\\\nand document set D = \u001b[0m\u001b[32m{\u001b[0m\u001b[32mDi\u001b[0m\u001b[32m}\u001b[0m\u001b[32mN\\\\\u001b[0m\u001b[32mni\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1\u001b[0m\u001b[32m, the goal of RAG is to give a answer\\\\n\\\\u02c6\\\\nAi for \u001b[0m\n",
       "\u001b[32meach query qi by\\\\nreferencing D\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component \u001b[0m\n",
       "\u001b[32mOptimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\n2.2\\\\nData Synthesis Optimization Methods\\\\nThe huge success of scaling law \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m in LLM has \u001b[0m\n",
       "\u001b[32mspurred data synthesis methods. In NLP \u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\\nmodel distillation from larger LLMs can be used to enhance the \u001b[0m\n",
       "\u001b[32mcapabilities of smaller LLMs.\\\\nIn LLM-based agents \u001b[0m\u001b[32m[\u001b[0m\u001b[32m43\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, the integration of LLMs with tools can be optimized using\u001b[0m\n",
       "\u001b[32msynthetic\\\\ncode \u001b[0m\u001b[32m[\u001b[0m\u001b[32m65\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and workflow \u001b[0m\u001b[32m[\u001b[0m\u001b[32m47, 63, 61\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In RAG, data synthesis methods encompass strategies for both\\\\nthe\u001b[0m\n",
       "\u001b[32mretriever \u001b[0m\u001b[32m[\u001b[0m\u001b[32m35, 28, 67\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and the LLM \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9, 23, 56, 6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. For the retriever, data synthesis focuses on\\\\ngenerating \u001b[0m\n",
       "\u001b[32mquery-positive-negative triplets using pure LLMs. Conversely, for the LLM itself, data\\\\nsynthesis aims to improve \u001b[0m\n",
       "\u001b[32mthe model\\\\u2019s ability to extract answers from retrieved texts\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust \u001b[0m\n",
       "\u001b[32mand Faithful RAG Component Optimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . For generators, maintaining fidelity to retrieved content is critical: \u001b[0m\n",
       "\u001b[32mwhen\\\\nfull clues are available, they should synthesize answers with precise citations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m; when clues \u001b[0m\n",
       "\u001b[32mare\\\\npartial, they should explicitly acknowledge gaps and provide only supported responses; in cases of \u001b[0m\n",
       "\u001b[32mno\\\\nrelevant clues, they should avoid speculative claims \u001b[0m\u001b[32m[\u001b[0m\u001b[32m48\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\\\nIn this paper, we propose RAGSYNTH, which \u001b[0m\n",
       "\u001b[32mincludes an RAG data synthesis modeling to capture\\\\nthe complex mapping relationships among documents, queries, \u001b[0m\n",
       "\u001b[32mground-truth answers, relevant clues,\\\\nand their connections to source documents, along with a specific \u001b[0m\n",
       "\u001b[32mimplementation. RAGSYNTH can\\\\nbe used for large-scale RAG data generation for arbitrary domain-specific documents,\u001b[0m\n",
       "\u001b[32msupporting\\\\nsingle-hop and multi-hop data, as well as sentence-level clue mapping. Using RAGSYNTH, we\\\\nsynthesize\u001b[0m\n",
       "\u001b[32ma dataset with diverse query logical complexities, clue completeness levels, and hop counts\\\\nto enhance retriever \u001b[0m\n",
       "\u001b[32mrobustness and generator fidelity\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from RAGSynth: Synthetic Data for Robust and Faithful RAG Component \u001b[0m\n",
       "\u001b[32mOptimization\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Various\\\\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\\\\nupon 2 \u001b[0m\n",
       "\u001b[32mcores: the retriever, which should robustly select relevant documents across\\\\ncomplex queries, and the generator, \u001b[0m\n",
       "\u001b[32mwhich should faithfully synthesize responses.\\\\nHowever, existing retrievers rely heavily on public knowledge and \u001b[0m\n",
       "\u001b[32mstruggle with\\\\nqueries of varying logical complexity and clue completeness, while generators\\\\nfrequently face \u001b[0m\n",
       "\u001b[32mfidelity problems. In this work, we introduce RAGSYNTH, a frame-\\\\nwork that includes a data construction modeling \u001b[0m\n",
       "\u001b[32mand a corresponding synthetic data\\\\ngeneration implementation, designed to optimize retriever robustness and \u001b[0m\n",
       "\u001b[32mgenerator\\\\nfidelity. Additionally, we present SYNTHBENCH, a benchmark encompassing 8\\\\ndomain-specific documents \u001b[0m\n",
       "\u001b[32macross 4 domains, featuring diverse query complexities,\\\\nclue completeness, and fine-grained citation granularity.\u001b[0m\n",
       "\u001b[32mLeveraging RAGSYNTH,\\\\nwe generate a large-scale synthetic dataset, including single and multi-hop\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer \u001b[0m\n",
       "\u001b[32monly from retrieval. Only cite sources that are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'We can conduct  data construction modeling and a corresponding synthetic data generation \u001b[0m\n",
       "\u001b[32mimplementation, designed to optimize retriever robustness and generator fidelity using RAg ?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Saving Your Index For Evaluation\n",
    "\n",
    "After you've implemented your RAG chain, please save your accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading). You'll have a chance to use it again for your final assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file (assuming the pip requirements are installed). After you have confirmed that the cell can pull in your index, download `docstore_index.tgz` for use in the last notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ". All other settings remain constant. In the\\nfollowing sections, we describe in more detail each one of the components and\\nprocesses used.\\n3.2\\nIndexing and retrieval\\nWe have used the open source system Weaviate7 as our vector database. As\\nencoder model, we have used a sentence transformer [35] trained on over 256M\\nquestions and answers, which is available from the HuggingFace system8.\\nAs shown in \\ufb01gure 2, to index a document, \\ufb01rst the document is split into\\nchunks, then each chunk is processed by an encoder model and then indexed into\\nthe vector database. Based on the chunking strategy a document will be split\\ninto a larger or smaller set of chunks.\\nFig. 2. Indexing of document chunks into the vector database\\n7 https://weaviate.io/developers/weaviate\\n8 https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-\\nv1\\n6\\nJimeno Yepes et al\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "Congratulations! Assuming your RAG chain is all good, you're now ready to move on to the **RAG Evaluation [Assessment]** section!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
